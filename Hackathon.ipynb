{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, BatchNormalization\n",
    "from keras.optimizers import rmsprop\n",
    "import keras.backend as K\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the game with the action space\n",
    "env = gym.make(\"MsPacman-v4\")\n",
    "action_space = [0,1,2,3,4,5,6,7,8] # All possible movements \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(render):\n",
    "    # Cuts off the bottom\n",
    "    render = render[:166]\n",
    "\n",
    "    # Cuts down the image to be smaller and less info, easier for ML to digest\n",
    "    render = render[::2,::2,0]\n",
    "    return render.astype(np.float)[:,:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rewards(r):\n",
    "    # Gamma is the value for the belmans equation\n",
    "    gamma = 0.75\n",
    "    discounted = np.zeros_like(r)\n",
    "    running_total = 0\n",
    "\n",
    "    for t in reversed(range(len(discounted))):\n",
    "        if r[t] != 0: running_add = 0\n",
    "        # Uses belmans equation to keep a cheep running total and to get momentum\n",
    "        running_total = r[t] + running_total * gamma\n",
    "        discounted[t] = running_total\n",
    "    \n",
    "    return discounted\n",
    "\n",
    "\n",
    "def discounted_reward(r):\n",
    "    # standardizes the reward by using z-scores \n",
    "    dr = rewards(r)\n",
    "    dr = (dr - dr.mean()) / dr.std()\n",
    "    return dr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model with conv2d layers to observe image\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size=(1,1), padding='same', activation='relu', input_shape = (83,80,1)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, kernel_size=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Setting the output size with softmax actication or a probabiity dist\n",
    "model.add(Dense(len(action_space), activation='softmax'))\n",
    "\n",
    "# RMSPROP seemed to work alot better tha adam, which seemed to die after a while\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating local var's\n",
    "episode = 0\n",
    "total_episodes = 1000\n",
    "reward_sums = np.zeros(total_episodes)\n",
    "loss = np.zeros(total_episodes)\n",
    "time_steps = np.zeros(total_episodes)\n",
    "\n",
    "running_reward = 0\n",
    "\n",
    "# Size of the game after it has been prepared\n",
    "input_size = (83, 80, 1)\n",
    "\n",
    "prev_frame = None\n",
    "steps = 15000 # How many action steps it allows befoe re starting the env\n",
    "\n",
    "# Creating arrays that are the size of the amount of steps that each game can take maxx\n",
    "xs = np.zeros((steps,)+input_size)\n",
    "ys = np.zeros((steps,1))\n",
    "rs = np.zeros((steps))\n",
    "\n",
    "# spin up env\n",
    "current_steps = 0\n",
    "observation = env.reset()\n",
    "\n",
    "while episode < total_episodes:\n",
    "    # env.render() # uncomment line to show the gameplay and learning\n",
    "    \n",
    "    # This gets the preprocessed difference of frames to be fed into the model\n",
    "    x = prepare(observation)\n",
    "    xs[current_steps] = x - prev_frame if prev_frame is not None else np.zeros(input_size)\n",
    "    prev_frame = x\n",
    "    \n",
    "    # This takes an action based on the current model, using random choice to stimuate the enviorment\n",
    "    p = model.predict(xs[current_steps][None,:,:,:])\n",
    "    a = np.random.choice(len(action_space), p=p[0]) \n",
    "    action = action_space[a] \n",
    "    ys[current_steps] = a #saves the action\n",
    "    \n",
    "    # Renew state of environment\n",
    "    observation, reward, done, info = env.step(action) #takes a step in the enviorment, getting a new set of varibles\n",
    "    running_reward += reward #record total rewards\n",
    "    rs[current_steps] = reward # record reward\n",
    "\n",
    "    current_steps += 1\n",
    "\n",
    "    if done or current_steps == steps:\n",
    "        reward_sums[episode] = running_reward\n",
    "        running_reward = 0\n",
    "\n",
    "        # Recording the episode frames, choices, and rewards to correct size\n",
    "        episode_x = xs[:current_steps]\n",
    "        episode_y = ys[:current_steps]\n",
    "        episode_r = rs[:current_steps]\n",
    "\n",
    "\n",
    "        # Discount and stanndardize rewards\n",
    "        episode_r = discounted_reward(episode_r)\n",
    "\n",
    "        # Train model\n",
    "        model.fit(episode_x,episode_y, sample_weight=episode_r, batch_size=512, epochs=1, verbose=0)\n",
    "\n",
    "        time_steps[episode] = current_steps\n",
    "\n",
    "        # Reset env for new game\n",
    "        current_steps = 0\n",
    "        prev_frame = None\n",
    "        observation = env.reset()\n",
    "\n",
    "        loss[episode] = model.evaluate(episode_x,episode_y, sample_weight=episode_r, batch_size=512, verbose=0)\n",
    "\n",
    "        # Iterate episode\n",
    "        episode += 1\n",
    "\n",
    "        # Prints 100 updates and saves it 100 times \n",
    "        if episode%(total_episodes//100) == 0:\n",
    "            avg_reward = np.mean(reward_sums[max(0,episode-200):episode])\n",
    "            avg_loss = np.mean(loss[max(0,episode-200):episode])\n",
    "            avg_time = np.mean(time_steps[max(0,episode-200):episode])\n",
    "            model.save('iLoveYoutNOT')\n",
    "            print(f\"{avg_reward} {avg_loss} {avg_time}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plays through with the model data \n",
    "import time\n",
    "observation = env.reset()\n",
    "env.render()\n",
    "\n",
    "# Pause to be able to get my window in the correct area\n",
    "time.sleep(5)\n",
    "\n",
    "# First frame so prev frame is none and it is not done\n",
    "prev_frame = None\n",
    "done = False\n",
    "while not done:\n",
    "\n",
    "    # prepare image and calculate diff of frames\n",
    "    x = prepare(observation) \n",
    "    diff = x - prev_frame if prev_frame is not None else np.zeros(input_size)\n",
    "\n",
    "    # Predict \n",
    "    p = model.predict(diff[None,:,:,:])\n",
    "    prev_frame = x\n",
    "    a = np.random.choice(len(action_space), p=p[0])\n",
    "    action = action_space[a]\n",
    "    \n",
    "    # Render into buffer. \n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    time.sleep(.02)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the model to make sure it works\n",
    "reload_model = keras.models.load_model('iLoveYoutNOT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48ea9879c8bb70bbd3f1491b1cad6ffd75ddf9f980e9f8cc9fea6e0fd9c709ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
