{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, BatchNormalization\n",
    "from keras.optimizers import rmsprop\n",
    "import keras.backend as K\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the game with the action space\n",
    "env = gym.make(\"MsPacman-v4\")\n",
    "action_space = [0,1,2,3,4,5,6,7,8] # All possible movements \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(render):\n",
    "    # Cuts off the bottom\n",
    "    render = render[:166]\n",
    "\n",
    "    # Cuts down the image to be smaller and less info, easier for ML to digest\n",
    "    render = render[::2,::2,0]\n",
    "    return render.astype(np.float)[:,:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rewards(r):\n",
    "    # Gamma is the value for the belmans equation\n",
    "    gamma = 0.75\n",
    "    discounted = np.zeros_like(r)\n",
    "    running_total = 0\n",
    "\n",
    "    for t in reversed(range(len(discounted))):\n",
    "        if r[t] != 0: running_add = 0\n",
    "        # Uses belmans equation to keep a cheep running total and to get momentum\n",
    "        running_total = r[t] + running_total * gamma\n",
    "        discounted[t] = running_total\n",
    "    \n",
    "    return discounted\n",
    "\n",
    "\n",
    "def discounted_reward(r):\n",
    "    # standardizes the reward by using z-scores \n",
    "    dr = rewards(r)\n",
    "    dr = (dr - dr.mean()) / dr.std()\n",
    "    return dr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3464: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2915: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating model with conv2d layers to observe image\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size=(1,1), padding='same', activation='relu', input_shape = (83,80,1)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, kernel_size=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Setting the output size with softmax actication or a probabiity dist\n",
    "model.add(Dense(len(action_space), activation='softmax'))\n",
    "\n",
    "# RMSPROP seemed to work alot better tha adam, which seemed to die after a while\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2378: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:159: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:173: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:182: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:189: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\gdgue\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:945: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "314.0 -0.375464635045125 796.0\n",
      "328.0 -0.2606655054145767 765.35\n",
      "335.0 -0.22350086265722752 744.5\n",
      "371.25 -0.19029911307721387 780.275\n",
      "380.6 -0.17677964520050776 792.14\n",
      "400.0 -0.16964681085125635 789.3666666666667\n",
      "399.7142857142857 -0.1639542481300238 786.8428571428572\n",
      "399.125 -0.15368137607167326 797.325\n",
      "395.55555555555554 -0.1496067949832614 790.1555555555556\n",
      "400.7 -0.14888389275287423 795.85\n",
      "418.27272727272725 -0.15614858094549441 794.3363636363637\n",
      "422.75 -0.16420821972793426 785.8583333333333\n",
      "425.84615384615387 -0.1599876157535034 784.1692307692308\n",
      "430.07142857142856 -0.16137067789615983 782.4714285714285\n",
      "431.8 -0.16891697053754878 776.9\n",
      "425.9375 -0.1744406007569319 766.36875\n",
      "425.4117647058824 -0.17822646663009958 760.0058823529412\n",
      "423.3888888888889 -0.18027943244522907 757.7222222222222\n",
      "422.3157894736842 -0.18086091350997072 759.5\n",
      "433.75 -0.17664334095668938 763.69\n",
      "441.85 -0.16664270482599247 761.925\n",
      "452.0 -0.1646102208833134 769.755\n",
      "463.15 -0.16349563995758856 777.9\n",
      "469.1 -0.16737249628275877 776.115\n",
      "476.95 -0.16503104130803542 777.59\n",
      "480.2 -0.16429015273388461 778.6\n",
      "492.1 -0.16504408699367987 780.85\n",
      "508.4 -0.16739080814537177 782.465\n",
      "523.45 -0.1670225265100586 788.865\n",
      "526.6 -0.16549293600335901 785.545\n",
      "519.95 -0.1614471557385324 789.84\n",
      "529.95 -0.15428939248400564 800.175\n",
      "530.25 -0.15578997054499996 804.835\n",
      "528.5 -0.1509297700962673 811.665\n",
      "528.45 -0.14164689942074524 822.87\n",
      "541.95 -0.13347894699739699 833.725\n",
      "550.8 -0.12827386215707798 844.475\n",
      "561.4 -0.12541577469370618 846.185\n",
      "576.9 -0.12108048860125913 851.44\n",
      "576.3 -0.12302626855815581 852.52\n",
      "582.25 -0.11986776308115388 856.1\n",
      "587.5 -0.1202223339962606 854.39\n",
      "599.5 -0.11721198289425297 855.075\n",
      "602.75 -0.11332922494784647 856.125\n",
      "609.65 -0.11611578869345263 852.875\n",
      "612.7 -0.11702526905038978 855.59\n",
      "605.1 -0.11579308319668531 853.2\n",
      "600.25 -0.11441109297846377 848.605\n",
      "600.2 -0.11515342775030973 847.205\n",
      "602.6 -0.11594061306681898 850.16\n",
      "618.85 -0.11280860486839255 851.545\n",
      "615.55 -0.1122838843508799 851.565\n",
      "618.25 -0.10825920912350007 855.44\n",
      "620.35 -0.11023311017427873 854.95\n",
      "625.8 -0.11049150450246692 849.435\n",
      "622.95 -0.109716508984156 851.565\n",
      "617.25 -0.10920675064564285 851.295\n",
      "613.65 -0.10651984637023178 856.835\n",
      "606.4 -0.10764719174519702 856.52\n",
      "615.7 -0.10626803137309113 858.7\n",
      "625.85 -0.10632885365370473 861.06\n",
      "625.1 -0.10714808406751093 862.035\n",
      "620.1 -0.10855192224819014 860.81\n",
      "616.1 -0.10975909634844407 858.825\n",
      "618.3 -0.1073383189896707 863.605\n",
      "613.6 -0.105480024070354 859.145\n",
      "618.2 -0.10473166265130335 861.98\n",
      "615.75 -0.10458961951821845 861.385\n",
      "609.05 -0.1045764357038276 860.195\n",
      "630.25 -0.10208657025509493 864.975\n",
      "613.3 -0.10368636601976057 858.735\n",
      "618.7 -0.10388896608536799 857.575\n",
      "617.8 -0.11087568616287104 850.765\n",
      "618.85 -0.11256746468025479 842.2\n",
      "621.25 -0.11698770784089894 841.11\n",
      "627.4 -0.11977094791197729 835.71\n",
      "628.15 -0.12167554270352962 830.615\n",
      "628.9 -0.12309079434988376 825.825\n",
      "611.55 -0.12276010960798604 812.895\n",
      "596.05 -0.12188666703274886 806.565\n",
      "585.95 -0.1201063705295379 804.375\n",
      "589.25 -0.11980669492737164 805.055\n",
      "591.85 -0.119630789419178 808.31\n",
      "595.95 -0.12016043494942168 809.915\n",
      "594.1 -0.12378794017832587 803.84\n",
      "600.65 -0.1271022653149429 807.445\n",
      "608.45 -0.12684940734572298 805.76\n",
      "614.5 -0.12695965585475777 808.09\n",
      "613.15 -0.1282682856109173 806.945\n",
      "590.4 -0.12988421512318038 801.355\n",
      "599.9 -0.1281337237557253 807.55\n",
      "595.1 -0.12930695417693813 804.99\n",
      "606.0 -0.12460340089181876 806.765\n",
      "601.55 -0.12253023522572562 806.895\n",
      "599.25 -0.11962875693502494 806.52\n",
      "591.65 -0.12092798655108455 804.045\n",
      "596.85 -0.11861391312312002 804.745\n",
      "597.5 -0.11929068296977458 803.185\n",
      "612.05 -0.11972928512698473 810.385\n",
      "615.6 -0.1213610246553099 810.415\n"
     ]
    }
   ],
   "source": [
    "# Creating local var's\n",
    "episode = 0\n",
    "total_episodes = 1000\n",
    "reward_sums = np.zeros(total_episodes)\n",
    "loss = np.zeros(total_episodes)\n",
    "time_steps = np.zeros(total_episodes)\n",
    "\n",
    "running_reward = 0\n",
    "\n",
    "# Size of the game after it has been prepared\n",
    "input_size = (83, 80, 1)\n",
    "\n",
    "prev_frame = None\n",
    "steps = 15000 # How many action steps it allows befoe re starting the env\n",
    "\n",
    "# Creating arrays that are the size of the amount of steps that each game can take maxx\n",
    "xs = np.zeros((steps,)+input_size)\n",
    "ys = np.zeros((steps,1))\n",
    "rs = np.zeros((steps))\n",
    "\n",
    "# spin up env\n",
    "current_steps = 0\n",
    "observation = env.reset()\n",
    "\n",
    "while episode < total_episodes:\n",
    "    # env.render() # uncomment line to show the gameplay and learning\n",
    "    \n",
    "    # This gets the preprocessed difference of frames to be fed into the model\n",
    "    x = prepare(observation)\n",
    "    xs[current_steps] = x - prev_frame if prev_frame is not None else np.zeros(input_size)\n",
    "    prev_frame = x\n",
    "    \n",
    "    # This takes an action based on the current model, using random choice to stimuate the enviorment\n",
    "    p = model.predict(xs[current_steps][None,:,:,:])\n",
    "    a = np.random.choice(len(action_space), p=p[0]) \n",
    "    action = action_space[a] \n",
    "    ys[current_steps] = a #saves the action\n",
    "    \n",
    "    # Renew state of environment\n",
    "    observation, reward, done, info = env.step(action) #takes a step in the enviorment, getting a new set of varibles\n",
    "    running_reward += reward #record total rewards\n",
    "    rs[current_steps] = reward # record reward\n",
    "\n",
    "    current_steps += 1\n",
    "\n",
    "    if done or current_steps == steps:\n",
    "        reward_sums[episode] = running_reward\n",
    "        running_reward = 0\n",
    "\n",
    "        # Recording the episode frames, choices, and rewards to correct size\n",
    "        episode_x = xs[:current_steps]\n",
    "        episode_y = ys[:current_steps]\n",
    "        episode_r = rs[:current_steps]\n",
    "\n",
    "\n",
    "        # Discount and stanndardize rewards\n",
    "        episode_r = discounted_reward(episode_r)\n",
    "\n",
    "        # Train model\n",
    "        model.fit(episode_x,episode_y, sample_weight=episode_r, batch_size=512, epochs=1, verbose=0)\n",
    "\n",
    "        time_steps[episode] = current_steps\n",
    "\n",
    "        # Reset env for new game\n",
    "        current_steps = 0\n",
    "        prev_frame = None\n",
    "        observation = env.reset()\n",
    "\n",
    "        loss[episode] = model.evaluate(episode_x,episode_y, sample_weight=episode_r, batch_size=512, verbose=0)\n",
    "\n",
    "        # Iterate episode\n",
    "        episode += 1\n",
    "\n",
    "        # Prints 100 updates and saves it 100 times \n",
    "        if episode%(total_episodes//100) == 0:\n",
    "            avg_reward = np.mean(reward_sums[max(0,episode-200):episode])\n",
    "            avg_loss = np.mean(loss[max(0,episode-200):episode])\n",
    "            avg_time = np.mean(time_steps[max(0,episode-200):episode])\n",
    "            model.save('iLoveYoutNOT')\n",
    "            print(f\"{avg_reward} {avg_loss} {avg_time}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plays through with the model data \n",
    "import time\n",
    "observation = env.reset()\n",
    "env.render()\n",
    "\n",
    "# Pause to be able to get my window in the correct area\n",
    "time.sleep(5)\n",
    "\n",
    "# First frame so prev frame is none and it is not done\n",
    "prev_frame = None\n",
    "done = False\n",
    "while not done:\n",
    "\n",
    "    # prepare image and calculate diff of frames\n",
    "    x = prepare(observation) \n",
    "    diff = x - prev_frame if prev_frame is not None else np.zeros(input_size)\n",
    "\n",
    "    # Predict \n",
    "    p = model.predict(diff[None,:,:,:])\n",
    "    prev_frame = x\n",
    "    a = np.random.choice(len(action_space), p=p[0])\n",
    "    action = action_space[a]\n",
    "    \n",
    "    # Render into buffer. \n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    time.sleep(.02)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the model to make sure it works\n",
    "reload_model = keras.models.load_model('iLoveYoutNOT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48ea9879c8bb70bbd3f1491b1cad6ffd75ddf9f980e9f8cc9fea6e0fd9c709ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
